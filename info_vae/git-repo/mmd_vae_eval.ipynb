{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some handy network layers\n",
    "def lrelu(x, rate=0.1):\n",
    "    return tf.maximum(tf.minimum(x * rate, 0), x)\n",
    "\n",
    "def conv2d_lrelu(inputs, num_outputs, kernel_size, stride):\n",
    "    conv = tf.contrib.layers.convolution2d(inputs, num_outputs, kernel_size, stride, \n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    conv = lrelu(conv)\n",
    "    return conv\n",
    "\n",
    "def conv2d_t_relu(inputs, num_outputs, kernel_size, stride):\n",
    "    conv = tf.contrib.layers.convolution2d_transpose(inputs, num_outputs, kernel_size, stride,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                     activation_fn=tf.identity)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "def fc_lrelu(inputs, num_outputs):\n",
    "    fc = tf.contrib.layers.fully_connected(inputs, num_outputs,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    fc = lrelu(fc)\n",
    "    return fc\n",
    "\n",
    "def fc_relu(inputs, num_outputs):\n",
    "    fc = tf.contrib.layers.fully_connected(inputs, num_outputs,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder and decoder use the DC-GAN architecture\n",
    "# 28 x 28 x 1\n",
    "def encoder(x, z_dim):\n",
    "    with tf.variable_scope('encoder'):\n",
    "        conv1 = conv2d_lrelu(x, 64, 4, 2)   # None x 14 x 14 x 64\n",
    "        conv2 = conv2d_lrelu(conv1, 128, 4, 2)   # None x 7 x 7 x 128\n",
    "        conv2 = tf.reshape(conv2, [-1, np.prod(conv2.get_shape().as_list()[1:])]) # None x (7x7x128)\n",
    "        fc1 = fc_lrelu(conv2, 1024)   \n",
    "        mean = tf.contrib.layers.fully_connected(fc1, z_dim, activation_fn=tf.identity)\n",
    "        stddev = tf.contrib.layers.fully_connected(fc1, z_dim, activation_fn=tf.sigmoid)\n",
    "        stddev = tf.maximum(stddev, 0.005)\n",
    "        return mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(z, reuse=False):\n",
    "    with tf.variable_scope('decoder') as vs:\n",
    "        if reuse:\n",
    "            vs.reuse_variables()\n",
    "        fc1 = fc_relu(z, 1024)\n",
    "        fc2 = fc_relu(fc1, 7*7*128)\n",
    "        fc2 = tf.reshape(fc2, tf.stack([tf.shape(fc2)[0], 7, 7, 128]))\n",
    "        conv1 = conv2d_t_relu(fc2, 64, 4, 2)\n",
    "        mean = tf.contrib.layers.convolution2d_transpose(conv1, 1, 4, 2, activation_fn=tf.sigmoid)\n",
    "        stddev = tf.contrib.layers.convolution2d_transpose(conv1, 1, 4, 2, activation_fn=tf.sigmoid)\n",
    "        stddev = tf.maximum(stddev, 0.005)\n",
    "        return mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the computation graph for training\n",
    "z_dim = 5\n",
    "x_dim = [28, 28, 1]\n",
    "train_x = tf.placeholder(tf.float32, shape=[None] + x_dim)\n",
    "train_zmean, train_zstddev = encoder(train_x, z_dim)\n",
    "train_z =  train_zmean + tf.multiply(train_zstddev,\n",
    "                                     tf.random_normal(tf.stack([tf.shape(train_x)[0], z_dim])))\n",
    "zstddev_logdet = tf.reduce_mean(tf.reduce_sum(2.0 * tf.log(train_zstddev), axis=1))\n",
    "\n",
    "train_xmean, train_xstddev = decoder(train_z)\n",
    "train_xr = train_xmean + tf.multiply(train_xstddev,\n",
    "                                     tf.random_normal(tf.stack([tf.shape(train_x)[0]] + x_dim)))\n",
    "xstddev_logdet = tf.reduce_mean(tf.reduce_sum(2.0 * tf.log(train_xstddev), axis=(1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the computation graph for generating samples\n",
    "gen_z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "gen_xmean, gen_xstddev = decoder(gen_z, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_nll = tf.div(tf.square(train_x - gen_xmean), tf.square(gen_xstddev)) / 2.0 + tf.log(gen_xstddev)\n",
    "sample_nll += math.log(2 * np.pi) / 2.0\n",
    "sample_nll = tf.reduce_sum(sample_nll, axis=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y):   # [batch_size, z_dim] [batch_size, z_dim]\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)\n",
    "\n",
    "# Compare the generated z with true samples from a standard Gaussian, and compute their MMD distance\n",
    "true_samples = tf.random_normal(tf.stack([200, z_dim]))\n",
    "loss_mmd = compute_mmd(true_samples, train_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_elbo = tf.reduce_sum(\n",
    "    -tf.log(train_zstddev) +\n",
    "    0.5 * tf.square(train_zstddev) +\n",
    "    0.5 * tf.square(train_zmean) - 0.5, axis=1\n",
    ")\n",
    "\n",
    "loss_elbo = tf.reduce_mean(loss_elbo) / np.prod(x_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_nll = tf.div(tf.square(train_x - train_xmean), tf.square(train_xstddev)) / 2.0 + tf.log(train_xstddev)\n",
    "loss_nll = tf.reduce_mean(loss_nll)\n",
    "loss_nll += math.log(2 * np.pi) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_nll + loss_elbo\n",
    "trainer = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('mnist_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a numpy array of shape [batch_size, height, width, 1] into a displayable array \n",
    "# of shape [height*sqrt(batch_size, width*sqrt(batch_size))] by tiling the images\n",
    "def convert_to_display(samples, max_samples=100):\n",
    "    if max_samples > samples.shape[0]:\n",
    "        max_samples = samples.shape[0]\n",
    "    cnt, height, width = int(math.floor(math.sqrt(max_samples))), samples.shape[1], samples.shape[2]\n",
    "    samples = samples[:cnt*cnt]\n",
    "    samples = np.transpose(samples, axes=[1, 0, 2, 3])\n",
    "    samples = np.reshape(samples, [height, cnt, cnt, width])\n",
    "    samples = np.transpose(samples, axes=[1, 0, 2, 3])\n",
    "    samples = np.reshape(samples, [height*cnt, width*cnt])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitedMnist:\n",
    "    def __init__(self, size):\n",
    "        self.data_ptr = 0\n",
    "        self.size = size\n",
    "        assert size <= mnist.train.images.shape[0]\n",
    "        self.data = mnist.train.images[:size]\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        prev_ptr = self.data_ptr\n",
    "        self.data_ptr += batch_size\n",
    "        if self.data_ptr > self.size:\n",
    "            prev_ptr = 0\n",
    "            self.data_ptr = batch_size\n",
    "        return self.data[prev_ptr:self.data_ptr]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limited_mnist = LimitedMnist(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    batch_x = limited_mnist.train_next_batch(100)\n",
    "    plt.imshow(convert_to_display(np.reshape(batch_x, [-1]+x_dim)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "batch_size = 100\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "for i in range(500000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    batch_x = batch_x.reshape(-1, 28, 28, 1)\n",
    "    _, nll, mmd, elbo, xmean, xstddev, xlogdet, zlogdet = \\\n",
    "        sess.run([trainer, loss_nll, loss_mmd, loss_elbo, train_xmean, train_xstddev, xstddev_logdet, zstddev_logdet],\n",
    "                 feed_dict={train_x: batch_x})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d, Negative log likelihood is %f, mmd loss %f, elbo loss %f\" % (i, nll, mmd, elbo * 784 / 5))\n",
    "        print(\"xlogdet %f  zlogdet %f\" % (xlogdet, zlogdet))\n",
    "    if i % 1000 == 0:\n",
    "        samples, sample_stddev = sess.run([gen_xmean, gen_xstddev], feed_dict={gen_z: np.random.normal(size=(100, z_dim))})\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(convert_to_display(samples), cmap='Greys_r')\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.imshow(convert_to_display(sample_stddev), cmap='Greys_r')\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.imshow(convert_to_display(xmean), cmap='Greys_r')\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.imshow(convert_to_display(xstddev), cmap='Greys_r')\n",
    "        plt.show()\n",
    "\n",
    "        z_list, label_list = [], []\n",
    "        test_batch_size = 500\n",
    "        for i in range(20):\n",
    "            batch_x, batch_y = mnist.test.next_batch(test_batch_size)\n",
    "            batch_x = batch_x.reshape(-1, 28, 28, 1)\n",
    "            z_list.append(sess.run(train_z, feed_dict={train_x: batch_x}))\n",
    "            label_list.append(batch_y)\n",
    "        z = np.concatenate(z_list, axis=0)\n",
    "        label = np.concatenate(label_list)\n",
    "        plt.scatter(z[:, 0], z[:, 1], c=label)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_sum(val):\n",
    "    min_val = np.min(val, axis=0, keepdims=True)\n",
    "    return np.mean(min_val - np.log(np.mean(np.exp(-val + min_val), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"---------------------> Computing true log likelihood\")\n",
    "start_time = time.time()\n",
    "avg_nll = []\n",
    "for _ in range(20):\n",
    "    # Takes about 40min per batch, expected to take 20h in total\n",
    "    batch_x, batch_y = mnist.test.next_batch(batch_size)\n",
    "    batch_x = np.reshape(batch_x, [-1] + x_dim)\n",
    "    nll_list = []\n",
    "    num_iter = 50000\n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        random_z = np.random.normal(size=[batch_size, z_dim])\n",
    "        nll = sess.run(sample_nll, feed_dict={train_x: batch_x, gen_z: random_z})\n",
    "        nll_list.append(nll)\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"%d %f, timed used %f\" % (iter, compute_log_sum(np.stack(nll_list)), time.time() - start_time))\n",
    "    nll = compute_log_sum(np.stack(nll_list))\n",
    "    print(\"Likelihood importance sampled = %f, time used %f\" % (nll, time.time() - start_time))\n",
    "    avg_nll.append(nll)\n",
    "nll = np.mean(avg_nll)\n",
    "print(\"Estimated log likelihood is %f, time elapsed %f\" % (nll, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"---------------------> Computing true log likelihood\")\n",
    "start_time = time.time()\n",
    "avg_nll = []\n",
    "for _ in range(20):\n",
    "    # Takes about 40min per batch, expected to take 20h in total\n",
    "    batch_x, batch_y = mnist.test.next_batch(batch_size)\n",
    "    batch_x = np.reshape(batch_x, [-1] + x_dim)\n",
    "    nll_list = []\n",
    "    num_iter = 50000\n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        random_z = np.random.normal(size=[batch_size, z_dim])\n",
    "        nll = sess.run(sample_nll, feed_dict={train_x: batch_x, gen_z: random_z})\n",
    "        nll_list.append(nll)\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"%d %f, timed used %f\" % (iter, compute_log_sum(np.stack(nll_list)), time.time() - start_time))\n",
    "    nll = compute_log_sum(np.stack(nll_list))\n",
    "    print(\"Likelihood importance sampled = %f, time used %f\" % (nll, time.time() - start_time))\n",
    "    avg_nll.append(nll)\n",
    "nll = np.mean(avg_nll)\n",
    "print(\"Estimated log likelihood is %f, time elapsed %f\" % (nll, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_z_extent():\n",
    "    z_list = []\n",
    "    for k in range(100):\n",
    "        batch_x = limited_mnist.next_batch(batch_size)\n",
    "        batch_x = np.reshape(batch_x, [-1]+x_dim)\n",
    "        z = sess.run(train_z, feed_dict={train_x: batch_x})\n",
    "        z_list.append(z)\n",
    "    z_list = np.concatenate(z_list, axis=0)\n",
    "    cov = np.cov(z_list.T)\n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "    return logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_z_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
